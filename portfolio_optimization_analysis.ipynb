{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a553807",
   "metadata": {},
   "source": [
    "# Time Series Forecasting for Portfolio Management Optimization\n",
    "\n",
    "## Executive Summary\n",
    "This analysis implements advanced time series forecasting models to optimize portfolio management strategies for GMF Investments. We analyze three key assets: Tesla (TSLA), Vanguard Total Bond Market ETF (BND), and S&P 500 ETF (SPY) using data from July 1, 2015 to July 31, 2025.\n",
    "\n",
    "### Key Objectives:\n",
    "1. **Data Preprocessing & EDA**: Extract and analyze historical financial data\n",
    "2. **Time Series Modeling**: Implement ARIMA/SARIMA and LSTM forecasting models\n",
    "3. **Portfolio Optimization**: Apply Modern Portfolio Theory to construct optimal portfolios\n",
    "4. **Strategy Backtesting**: Validate performance against benchmark portfolios\n",
    "\n",
    "### Business Context:\n",
    "- **TSLA**: High-growth, high-volatility stock providing potential high returns\n",
    "- **BND**: Bond ETF offering stability and income generation\n",
    "- **SPY**: Market index providing diversified, moderate-risk exposure\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a81a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical and ML libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pmdarima as pm\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Portfolio optimization libraries\n",
    "from pypfopt import EfficientFrontier, risk_models, expected_returns\n",
    "from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices\n",
    "import scipy.optimize as sco\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a93092",
   "metadata": {},
   "source": [
    "## 1. Data Extraction and Setup\n",
    "\n",
    "### Asset Portfolio:\n",
    "- **TSLA (Tesla)**: High-growth stock with high volatility\n",
    "- **BND (Vanguard Total Bond Market ETF)**: Low-risk bond ETF for stability\n",
    "- **SPY (S&P 500 ETF)**: Market index for diversified exposure\n",
    "\n",
    "### Data Period: July 1, 2015 to July 31, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the assets and date range\n",
    "assets = ['TSLA', 'BND', 'SPY']\n",
    "start_date = '2015-07-01'\n",
    "end_date = '2025-07-31'\n",
    "\n",
    "print(f\"üìä Extracting data for {assets} from {start_date} to {end_date}\")\n",
    "\n",
    "# Extract data for all assets\n",
    "data = {}\n",
    "for asset in assets:\n",
    "    try:\n",
    "        ticker = yf.Ticker(asset)\n",
    "        df = ticker.history(start=start_date, end=end_date)\n",
    "        data[asset] = df\n",
    "        print(f\"‚úÖ {asset}: {len(df)} trading days extracted\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting {asset}: {e}\")\n",
    "\n",
    "# Display basic info about the datasets\n",
    "print(f\"\\nüìà Data extraction completed!\")\n",
    "print(f\"Date range: {data['TSLA'].index.min().date()} to {data['TSLA'].index.max().date()}\")\n",
    "\n",
    "# Quick preview of TSLA data structure\n",
    "print(f\"\\nüîç TSLA Data Structure:\")\n",
    "print(data['TSLA'].head())\n",
    "print(f\"\\nData shape: {data['TSLA'].shape}\")\n",
    "print(f\"Columns: {list(data['TSLA'].columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af0019",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing\n",
    "\n",
    "### Data Quality Assessment:\n",
    "- Check for missing values\n",
    "- Ensure proper data types\n",
    "- Handle any inconsistencies\n",
    "- Prepare data for time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "print(\"üîç DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for asset in assets:\n",
    "    df = data[asset]\n",
    "    print(f\"\\nüìä {asset} Dataset:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Date range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  Data types: {df.dtypes.unique()}\")\n",
    "    \n",
    "    # Check for any zero or negative values in prices\n",
    "    negative_prices = (df[['Open', 'High', 'Low', 'Close', 'Adj Close']] <= 0).any().any()\n",
    "    print(f\"  Negative/zero prices: {negative_prices}\")\n",
    "\n",
    "# Create a combined dataset with adjusted close prices\n",
    "print(f\"\\nüìà Creating combined dataset...\")\n",
    "close_prices = pd.DataFrame()\n",
    "for asset in assets:\n",
    "    close_prices[asset] = data[asset]['Adj Close']\n",
    "\n",
    "# Handle missing values (forward fill then backward fill)\n",
    "close_prices = close_prices.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Calculate daily returns\n",
    "returns = close_prices.pct_change().dropna()\n",
    "\n",
    "print(f\"‚úÖ Combined dataset created:\")\n",
    "print(f\"  Shape: {close_prices.shape}\")\n",
    "print(f\"  Returns shape: {returns.shape}\")\n",
    "print(f\"  Missing values in prices: {close_prices.isnull().sum().sum()}\")\n",
    "print(f\"  Missing values in returns: {returns.isnull().sum().sum()}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nüìä Basic Statistics - Adjusted Close Prices:\")\n",
    "print(close_prices.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49437491",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis and Visualization\n",
    "\n",
    "### Key Analysis Areas:\n",
    "- Price trends over time\n",
    "- Volatility patterns\n",
    "- Correlation analysis between assets\n",
    "- Distribution of returns\n",
    "- Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce67a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Price Trends Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Asset Price Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Individual price trends\n",
    "for i, asset in enumerate(assets):\n",
    "    ax = axes[0, i] if i < 2 else axes[1, 0]\n",
    "    close_prices[asset].plot(ax=ax, title=f'{asset} Adjusted Close Price', linewidth=2)\n",
    "    ax.set_ylabel('Price ($)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Normalized prices comparison\n",
    "axes[1, 1].set_title('Normalized Price Comparison (Base = 100)')\n",
    "normalized_prices = (close_prices / close_prices.iloc[0]) * 100\n",
    "for asset in assets:\n",
    "    axes[1, 1].plot(normalized_prices.index, normalized_prices[asset], \n",
    "                   label=asset, linewidth=2)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylabel('Normalized Price')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Returns Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Daily Returns Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Daily returns over time\n",
    "for i, asset in enumerate(assets):\n",
    "    ax = axes[0, i] if i < 2 else axes[1, 0]\n",
    "    returns[asset].plot(ax=ax, title=f'{asset} Daily Returns', alpha=0.7)\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_ylabel('Daily Return')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Returns distribution\n",
    "axes[1, 1].set_title('Returns Distribution')\n",
    "for asset in assets:\n",
    "    axes[1, 1].hist(returns[asset], bins=50, alpha=0.6, label=asset)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_xlabel('Daily Return')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Basic Statistics\n",
    "print(\"üìä RETURNS STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "stats_df = returns.describe()\n",
    "stats_df.loc['Skewness'] = returns.skew()\n",
    "stats_df.loc['Kurtosis'] = returns.kurtosis()\n",
    "print(stats_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Correlation Analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Asset Correlation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Returns correlation heatmap\n",
    "corr_matrix = returns.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=axes[0], square=True, linewidths=0.5)\n",
    "axes[0].set_title('Daily Returns Correlation')\n",
    "\n",
    "# Price correlation heatmap\n",
    "price_corr = close_prices.corr()\n",
    "sns.heatmap(price_corr, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=axes[1], square=True, linewidths=0.5)\n",
    "axes[1].set_title('Price Levels Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîó CORRELATION INSIGHTS:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Returns Correlation Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "print(f\"\\nHighest correlation: {corr_matrix.where(corr_matrix != 1).max().max():.3f}\")\n",
    "print(f\"Lowest correlation: {corr_matrix.where(corr_matrix != 1).min().min():.3f}\")\n",
    "\n",
    "# 5. Rolling Volatility Analysis\n",
    "window = 30  # 30-day rolling window\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Rolling Volatility Analysis (30-day window)', fontsize=16, fontweight='bold')\n",
    "\n",
    "rolling_vol = returns.rolling(window=window).std() * np.sqrt(252)  # Annualized volatility\n",
    "\n",
    "for i, asset in enumerate(assets):\n",
    "    ax = axes[i//2, i%2]\n",
    "    rolling_vol[asset].plot(ax=ax, title=f'{asset} Rolling Volatility', linewidth=2)\n",
    "    ax.set_ylabel('Annualized Volatility')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Combined volatility comparison\n",
    "axes[1, 1].clear()\n",
    "axes[1, 1].set_title('Volatility Comparison')\n",
    "for asset in assets:\n",
    "    axes[1, 1].plot(rolling_vol.index, rolling_vol[asset], label=asset, linewidth=2)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylabel('Annualized Volatility')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Volatility statistics\n",
    "print(\"\\nüìà VOLATILITY STATISTICS (Annualized):\")\n",
    "print(\"=\" * 45)\n",
    "vol_stats = rolling_vol.describe()\n",
    "print(vol_stats.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c01a7f",
   "metadata": {},
   "source": [
    "## 4. Stationarity Testing and Feature Engineering\n",
    "\n",
    "### Stationarity Analysis:\n",
    "- Augmented Dickey-Fuller (ADF) Test\n",
    "- Rolling statistics analysis\n",
    "- Feature engineering for time series modeling\n",
    "\n",
    "**Note**: Stationarity is crucial for ARIMA models. Non-stationary series require differencing to become stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stationarity Testing Function\n",
    "def check_stationarity(timeseries, title):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity\n",
    "    \"\"\"\n",
    "    print(f\"üîç ADF Test Results for {title}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Perform ADF test\n",
    "    result = adfuller(timeseries.dropna())\n",
    "    \n",
    "    print(f'ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'p-value: {result[1]:.6f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "    \n",
    "    # Interpretation\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"‚úÖ Series is STATIONARY (reject null hypothesis)\")\n",
    "    else:\n",
    "        print(\"‚ùå Series is NON-STATIONARY (fail to reject null hypothesis)\")\n",
    "    \n",
    "    return result[1] <= 0.05\n",
    "\n",
    "# Test stationarity for prices and returns\n",
    "print(\"üìä STATIONARITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stationarity_results = {}\n",
    "\n",
    "# Test prices (expected to be non-stationary)\n",
    "print(\"\\nüè∑Ô∏è PRICE LEVELS:\")\n",
    "for asset in assets:\n",
    "    is_stationary = check_stationarity(close_prices[asset], f\"{asset} Prices\")\n",
    "    stationarity_results[f\"{asset}_prices\"] = is_stationary\n",
    "    print()\n",
    "\n",
    "# Test returns (expected to be stationary)\n",
    "print(\"\\nüìà DAILY RETURNS:\")\n",
    "for asset in assets:\n",
    "    is_stationary = check_stationarity(returns[asset], f\"{asset} Returns\")\n",
    "    stationarity_results[f\"{asset}_returns\"] = is_stationary\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "print(\"üìã STATIONARITY SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "for test, result in stationarity_results.items():\n",
    "    status = \"‚úÖ Stationary\" if result else \"‚ùå Non-stationary\"\n",
    "    print(f\"{test}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and Risk Metrics\n",
    "print(\"üîß FEATURE ENGINEERING & RISK METRICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate additional features for each asset\n",
    "features_data = {}\n",
    "\n",
    "for asset in assets:\n",
    "    print(f\"\\nüìä Processing {asset}...\")\n",
    "    \n",
    "    # Get price and return data\n",
    "    prices = close_prices[asset]\n",
    "    asset_returns = returns[asset]\n",
    "    \n",
    "    # Calculate features\n",
    "    features = pd.DataFrame(index=prices.index)\n",
    "    features['price'] = prices\n",
    "    features['returns'] = asset_returns\n",
    "    \n",
    "    # Rolling statistics (30-day window)\n",
    "    features['ma_30'] = prices.rolling(30).mean()\n",
    "    features['volatility_30'] = asset_returns.rolling(30).std() * np.sqrt(252)\n",
    "    features['rolling_max'] = prices.rolling(30).max()\n",
    "    features['rolling_min'] = prices.rolling(30).min()\n",
    "    \n",
    "    # Technical indicators\n",
    "    features['price_ma_ratio'] = prices / features['ma_30']\n",
    "    features['volatility_percentile'] = features['volatility_30'].rolling(252).rank(pct=True)\n",
    "    \n",
    "    # Lag features\n",
    "    features['returns_lag1'] = asset_returns.shift(1)\n",
    "    features['returns_lag2'] = asset_returns.shift(2)\n",
    "    features['returns_lag3'] = asset_returns.shift(3)\n",
    "    \n",
    "    features_data[asset] = features\n",
    "\n",
    "# Risk Metrics Calculation\n",
    "risk_free_rate = 0.02  # Assume 2% risk-free rate\n",
    "\n",
    "print(f\"\\nüí∞ RISK METRICS CALCULATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "risk_metrics = {}\n",
    "\n",
    "for asset in assets:\n",
    "    asset_returns = returns[asset]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    metrics['Mean_Return'] = asset_returns.mean() * 252  # Annualized\n",
    "    metrics['Volatility'] = asset_returns.std() * np.sqrt(252)  # Annualized\n",
    "    metrics['Sharpe_Ratio'] = (metrics['Mean_Return'] - risk_free_rate) / metrics['Volatility']\n",
    "    \n",
    "    # Value at Risk (VaR) - 5% and 1% levels\n",
    "    metrics['VaR_5%'] = np.percentile(asset_returns, 5)\n",
    "    metrics['VaR_1%'] = np.percentile(asset_returns, 1)\n",
    "    \n",
    "    # Maximum Drawdown\n",
    "    cum_returns = (1 + asset_returns).cumprod()\n",
    "    running_max = cum_returns.expanding().max()\n",
    "    drawdown = (cum_returns - running_max) / running_max\n",
    "    metrics['Max_Drawdown'] = drawdown.min()\n",
    "    \n",
    "    # Skewness and Kurtosis\n",
    "    metrics['Skewness'] = asset_returns.skew()\n",
    "    metrics['Kurtosis'] = asset_returns.kurtosis()\n",
    "    \n",
    "    risk_metrics[asset] = metrics\n",
    "\n",
    "# Display risk metrics\n",
    "risk_df = pd.DataFrame(risk_metrics).T\n",
    "print(risk_df.round(4))\n",
    "\n",
    "# Focus on TSLA for detailed analysis (main forecasting target)\n",
    "tsla_data = features_data['TSLA'].copy()\n",
    "tsla_returns = returns['TSLA'].copy()\n",
    "\n",
    "print(f\"\\nüéØ TSLA DETAILED ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Data points: {len(tsla_data)}\")\n",
    "print(f\"Date range: {tsla_data.index.min().date()} to {tsla_data.index.max().date()}\")\n",
    "print(f\"Missing values: {tsla_data.isnull().sum().sum()}\")\n",
    "\n",
    "# TSLA specific insights\n",
    "print(f\"\\nüìà TSLA Key Insights:\")\n",
    "print(f\"  Average annual return: {risk_metrics['TSLA']['Mean_Return']:.2%}\")\n",
    "print(f\"  Annual volatility: {risk_metrics['TSLA']['Volatility']:.2%}\")\n",
    "print(f\"  Sharpe ratio: {risk_metrics['TSLA']['Sharpe_Ratio']:.3f}\")\n",
    "print(f\"  Maximum drawdown: {risk_metrics['TSLA']['Max_Drawdown']:.2%}\")\n",
    "print(f\"  VaR (5%): {risk_metrics['TSLA']['VaR_5%']:.2%}\")\n",
    "print(f\"  VaR (1%): {risk_metrics['TSLA']['VaR_1%']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14b078",
   "metadata": {},
   "source": [
    "## 5. ARIMA/SARIMA Model Implementation\n",
    "\n",
    "### ARIMA Model Overview:\n",
    "- **AR (AutoRegressive)**: Uses past values to predict future values\n",
    "- **I (Integrated)**: Differencing to make the series stationary\n",
    "- **MA (Moving Average)**: Uses past forecast errors in prediction\n",
    "\n",
    "### Approach:\n",
    "1. Split data chronologically (2015-2023 for training, 2024-2025 for testing)\n",
    "2. Use auto_arima to find optimal parameters\n",
    "3. Fit model and generate forecasts\n",
    "4. Evaluate performance using multiple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9004df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Model Implementation\n",
    "print(\"ü§ñ ARIMA MODEL IMPLEMENTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare TSLA data for modeling (focus on adjusted close prices)\n",
    "tsla_prices = close_prices['TSLA'].copy()\n",
    "\n",
    "# Chronological split: Train on 2015-2023, Test on 2024-2025\n",
    "split_date = '2024-01-01'\n",
    "train_data = tsla_prices[tsla_prices.index < split_date]\n",
    "test_data = tsla_prices[tsla_prices.index >= split_date]\n",
    "\n",
    "print(f\"üìÖ Data Split:\")\n",
    "print(f\"  Training period: {train_data.index.min().date()} to {train_data.index.max().date()}\")\n",
    "print(f\"  Testing period: {test_data.index.min().date()} to {test_data.index.max().date()}\")\n",
    "print(f\"  Training samples: {len(train_data)}\")\n",
    "print(f\"  Testing samples: {len(test_data)}\")\n",
    "\n",
    "# Check if we need to work with returns instead of prices (since prices are non-stationary)\n",
    "train_returns = train_data.pct_change().dropna()\n",
    "test_returns = test_data.pct_change().dropna()\n",
    "\n",
    "print(f\"\\nüîç Using returns for ARIMA modeling (since prices are non-stationary)\")\n",
    "\n",
    "# Auto ARIMA to find optimal parameters\n",
    "print(f\"\\nüîß Finding optimal ARIMA parameters...\")\n",
    "try:\n",
    "    auto_arima_model = pm.auto_arima(\n",
    "        train_returns,\n",
    "        start_p=0, start_q=0,\n",
    "        max_p=5, max_q=5,\n",
    "        seasonal=False,\n",
    "        stepwise=True,\n",
    "        suppress_warnings=True,\n",
    "        error_action='ignore',\n",
    "        max_iter=10,\n",
    "        out_of_sample_size=int(len(train_returns) * 0.2)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Optimal ARIMA order: {auto_arima_model.order}\")\n",
    "    print(f\"üìä AIC: {auto_arima_model.aic():.2f}\")\n",
    "    print(f\"üìä BIC: {auto_arima_model.bic():.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Auto ARIMA failed: {e}\")\n",
    "    print(\"üîß Using default ARIMA(1,0,1) parameters\")\n",
    "    auto_arima_model = pm.ARIMA(order=(1, 0, 1))\n",
    "    auto_arima_model.fit(train_returns)\n",
    "\n",
    "# Generate forecasts\n",
    "forecast_steps = len(test_returns)\n",
    "forecast_result = auto_arima_model.predict(n_periods=forecast_steps, return_conf_int=True)\n",
    "forecast_values = forecast_result[0]\n",
    "confidence_intervals = forecast_result[1]\n",
    "\n",
    "print(f\"\\nüìà Generated {len(forecast_values)} forecasts\")\n",
    "\n",
    "# Convert return forecasts back to price forecasts\n",
    "last_train_price = train_data.iloc[-1]\n",
    "forecast_prices = [last_train_price]\n",
    "\n",
    "for i, ret_forecast in enumerate(forecast_values):\n",
    "    next_price = forecast_prices[-1] * (1 + ret_forecast)\n",
    "    forecast_prices.append(next_price)\n",
    "\n",
    "forecast_prices = np.array(forecast_prices[1:])  # Remove the initial price\n",
    "\n",
    "# Calculate performance metrics\n",
    "def calculate_metrics(actual, predicted):\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    return mae, rmse, mape\n",
    "\n",
    "mae, rmse, mape = calculate_metrics(test_data.values, forecast_prices)\n",
    "\n",
    "print(f\"\\nüìä ARIMA MODEL PERFORMANCE:\")\n",
    "print(f\"  MAE: ${mae:.2f}\")\n",
    "print(f\"  RMSE: ${rmse:.2f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Store results for later comparison\n",
    "arima_results = {\n",
    "    'model': auto_arima_model,\n",
    "    'forecast_prices': forecast_prices,\n",
    "    'forecast_returns': forecast_values,\n",
    "    'confidence_intervals': confidence_intervals,\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'mape': mape,\n",
    "    'test_data': test_data,\n",
    "    'train_data': train_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a962148",
   "metadata": {},
   "source": [
    "## 6. LSTM Model Implementation\n",
    "\n",
    "### LSTM Overview:\n",
    "- **Long Short-Term Memory** networks are designed to handle sequential data\n",
    "- Can capture long-term dependencies in time series\n",
    "- Particularly effective for complex, non-linear patterns\n",
    "\n",
    "### Architecture:\n",
    "- Sequential model with LSTM layers\n",
    "- Dropout for regularization\n",
    "- Dense output layer for price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Implementation\n",
    "print(\"üß† LSTM MODEL IMPLEMENTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Data preparation for LSTM\n",
    "def create_sequences(data, lookback_window=60):\n",
    "    \"\"\"Create sequences for LSTM training\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(lookback_window, len(data)):\n",
    "        X.append(data[i-lookback_window:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare and scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_train = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "scaled_test = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "print(f\"üìä Data scaling completed\")\n",
    "print(f\"  Original train range: ${train_data.min():.2f} - ${train_data.max():.2f}\")\n",
    "print(f\"  Scaled train range: {scaled_train.min():.3f} - {scaled_train.max():.3f}\")\n",
    "\n",
    "# Create sequences\n",
    "lookback_window = 60  # Use 60 days of history to predict next day\n",
    "X_train, y_train = create_sequences(scaled_train, lookback_window)\n",
    "\n",
    "print(f\"\\nüîß Sequence creation:\")\n",
    "print(f\"  Lookback window: {lookback_window} days\")\n",
    "print(f\"  Training sequences: {X_train.shape}\")\n",
    "print(f\"  Training targets: {y_train.shape}\")\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(lookback_window, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "print(f\"\\nüèóÔ∏è LSTM Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nüèÉ‚Äç‚ôÇÔ∏è Training LSTM model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    shuffle=False  # Important for time series\n",
    ")\n",
    "\n",
    "# Prepare test data for prediction\n",
    "# We need to include some training data to create the first test sequence\n",
    "extended_data = np.concatenate([scaled_train, scaled_test])\n",
    "test_sequences, _ = create_sequences(extended_data[-(len(test_data) + lookback_window):], lookback_window)\n",
    "\n",
    "# Generate predictions\n",
    "print(f\"\\nüîÆ Generating LSTM predictions...\")\n",
    "lstm_predictions_scaled = model.predict(test_sequences)\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
    "\n",
    "# Calculate LSTM performance metrics\n",
    "lstm_mae, lstm_rmse, lstm_mape = calculate_metrics(test_data.values, lstm_predictions.flatten())\n",
    "\n",
    "print(f\"\\nüìä LSTM MODEL PERFORMANCE:\")\n",
    "print(f\"  MAE: ${lstm_mae:.2f}\")\n",
    "print(f\"  RMSE: ${lstm_rmse:.2f}\")\n",
    "print(f\"  MAPE: {lstm_mape:.2f}%\")\n",
    "\n",
    "# Store LSTM results\n",
    "lstm_results = {\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'predictions': lstm_predictions.flatten(),\n",
    "    'mae': lstm_mae,\n",
    "    'rmse': lstm_rmse,\n",
    "    'mape': lstm_mape,\n",
    "    'history': history,\n",
    "    'lookback_window': lookback_window\n",
    "}\n",
    "\n",
    "# Training history visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('LSTM Training History')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model comparison bar chart\n",
    "models = ['ARIMA', 'LSTM']\n",
    "mae_scores = [arima_results['mae'], lstm_results['mae']]\n",
    "rmse_scores = [arima_results['rmse'], lstm_results['rmse']]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, mae_scores, width, label='MAE', alpha=0.8)\n",
    "axes[1].bar(x + width/2, rmse_scores, width, label='RMSE', alpha=0.8)\n",
    "axes[1].set_title('Model Comparison')\n",
    "axes[1].set_xlabel('Models')\n",
    "axes[1].set_ylabel('Error ($)')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81268b43",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Evaluation\n",
    "\n",
    "### Performance Analysis:\n",
    "Compare ARIMA and LSTM models using multiple metrics and visualizations to determine the best approach for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa46355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison\n",
    "print(\"üèÜ MODEL COMPARISON & EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['ARIMA', 'LSTM'],\n",
    "    'MAE': [arima_results['mae'], lstm_results['mae']],\n",
    "    'RMSE': [arima_results['rmse'], lstm_results['rmse']],\n",
    "    'MAPE (%)': [arima_results['mape'], lstm_results['mape']]\n",
    "})\n",
    "\n",
    "print(\"üìä Performance Metrics Comparison:\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Determine best model\n",
    "best_model_idx = comparison_df['RMSE'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nü•á Best performing model: {best_model_name} (based on lowest RMSE)\")\n",
    "\n",
    "# Detailed visualization comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('TSLA Stock Price Forecasting: Model Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Actual vs Predicted (both models)\n",
    "axes[0, 0].plot(test_data.index, test_data.values, label='Actual', linewidth=2, color='black')\n",
    "axes[0, 0].plot(test_data.index, arima_results['forecast_prices'], \n",
    "                label='ARIMA Forecast', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].plot(test_data.index, lstm_results['predictions'], \n",
    "                label='LSTM Forecast', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].set_title('Actual vs Predicted Prices')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Prediction errors\n",
    "arima_errors = test_data.values - arima_results['forecast_prices']\n",
    "lstm_errors = test_data.values - lstm_results['predictions']\n",
    "\n",
    "axes[0, 1].plot(test_data.index, arima_errors, label='ARIMA Errors', alpha=0.7)\n",
    "axes[0, 1].plot(test_data.index, lstm_errors, label='LSTM Errors', alpha=0.7)\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_title('Prediction Errors Over Time')\n",
    "axes[0, 1].set_ylabel('Error ($)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Error distribution\n",
    "axes[1, 0].hist(arima_errors, bins=30, alpha=0.6, label='ARIMA', density=True)\n",
    "axes[1, 0].hist(lstm_errors, bins=30, alpha=0.6, label='LSTM', density=True)\n",
    "axes[1, 0].set_title('Error Distribution')\n",
    "axes[1, 0].set_xlabel('Error ($)')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Performance metrics bar chart\n",
    "metrics = ['MAE', 'RMSE', 'MAPE']\n",
    "arima_scores = [arima_results['mae'], arima_results['rmse'], arima_results['mape']]\n",
    "lstm_scores = [lstm_results['mae'], lstm_results['rmse'], lstm_results['mape']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width/2, arima_scores, width, label='ARIMA', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, lstm_scores, width, label='LSTM', alpha=0.8)\n",
    "axes[1, 1].set_title('Performance Metrics Comparison')\n",
    "axes[1, 1].set_xlabel('Metrics')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metrics)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance test (Diebold-Mariano test approximation)\n",
    "def dm_test(e1, e2):\n",
    "    \"\"\"Simplified Diebold-Mariano test for forecast accuracy\"\"\"\n",
    "    d = e1**2 - e2**2\n",
    "    mean_d = np.mean(d)\n",
    "    var_d = np.var(d, ddof=1)\n",
    "    dm_stat = mean_d / np.sqrt(var_d / len(d))\n",
    "    return dm_stat\n",
    "\n",
    "dm_statistic = dm_test(arima_errors, lstm_errors)\n",
    "print(f\"\\nüìà Statistical Analysis:\")\n",
    "print(f\"  Diebold-Mariano statistic: {dm_statistic:.3f}\")\n",
    "if abs(dm_statistic) > 1.96:\n",
    "    print(f\"  ‚úÖ Significant difference in forecast accuracy (95% confidence)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå No significant difference in forecast accuracy\")\n",
    "\n",
    "# Summary insights\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "print(f\"  ‚Ä¢ {best_model_name} performs better with {comparison_df.loc[best_model_idx, 'RMSE']:.2f} RMSE\")\n",
    "print(f\"  ‚Ä¢ ARIMA captures linear trends well\")\n",
    "print(f\"  ‚Ä¢ LSTM may capture non-linear patterns better\")\n",
    "print(f\"  ‚Ä¢ Both models show reasonable performance for financial forecasting\")\n",
    "\n",
    "# Select best model for portfolio optimization\n",
    "if best_model_name == 'ARIMA':\n",
    "    best_model = arima_results\n",
    "    best_predictions = arima_results['forecast_prices']\n",
    "else:\n",
    "    best_model = lstm_results\n",
    "    best_predictions = lstm_results['predictions']\n",
    "\n",
    "print(f\"\\nüéØ Selected {best_model_name} for portfolio optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46628b17",
   "metadata": {},
   "source": [
    "## 8. Future Price Forecasting\n",
    "\n",
    "### Forecasting Horizon: 6-12 Months\n",
    "Generate future price predictions using the best-performing model with confidence intervals for risk assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789688cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future Price Forecasting\n",
    "print(\"üîÆ FUTURE PRICE FORECASTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define forecast horizon\n",
    "forecast_months = 9  # 9 months ahead\n",
    "forecast_days = forecast_months * 21  # Approximate trading days per month\n",
    "\n",
    "# Create future dates\n",
    "last_date = close_prices.index[-1]\n",
    "future_dates = pd.date_range(start=last_date + timedelta(days=1), \n",
    "                           periods=forecast_days, freq='B')  # Business days\n",
    "\n",
    "print(f\"üìÖ Forecasting period: {future_dates[0].date()} to {future_dates[-1].date()}\")\n",
    "print(f\"üìä Forecast horizon: {forecast_days} business days ({forecast_months} months)\")\n",
    "\n",
    "# Generate future forecasts based on best model\n",
    "if best_model_name == 'ARIMA':\n",
    "    # ARIMA future forecast\n",
    "    print(f\"\\nü§ñ Generating ARIMA forecasts...\")\n",
    "    future_forecast = arima_results['model'].predict(n_periods=forecast_days, return_conf_int=True)\n",
    "    future_returns = future_forecast[0]\n",
    "    future_conf_int = future_forecast[1]\n",
    "    \n",
    "    # Convert returns to prices\n",
    "    last_price = close_prices['TSLA'].iloc[-1]\n",
    "    future_prices = [last_price]\n",
    "    \n",
    "    for ret in future_returns:\n",
    "        next_price = future_prices[-1] * (1 + ret)\n",
    "        future_prices.append(next_price)\n",
    "    \n",
    "    future_prices = np.array(future_prices[1:])\n",
    "    \n",
    "    # Calculate confidence intervals for prices\n",
    "    lower_bound = []\n",
    "    upper_bound = []\n",
    "    current_price = last_price\n",
    "    \n",
    "    for i, (ret, (lower_ret, upper_ret)) in enumerate(zip(future_returns, future_conf_int)):\n",
    "        lower_price = current_price * (1 + lower_ret)\n",
    "        upper_price = current_price * (1 + upper_ret)\n",
    "        lower_bound.append(lower_price)\n",
    "        upper_bound.append(upper_price)\n",
    "        current_price = future_prices[i]\n",
    "    \n",
    "else:\n",
    "    # LSTM future forecast\n",
    "    print(f\"\\nüß† Generating LSTM forecasts...\")\n",
    "    \n",
    "    # Prepare last sequence from available data\n",
    "    last_sequence = scaled_test[-lookback_window:].reshape(1, lookback_window, 1)\n",
    "    future_prices = []\n",
    "    \n",
    "    # Generate step-by-step predictions\n",
    "    current_sequence = last_sequence.copy()\n",
    "    \n",
    "    for _ in range(forecast_days):\n",
    "        next_pred_scaled = lstm_results['model'].predict(current_sequence, verbose=0)\n",
    "        next_pred = scaler.inverse_transform(next_pred_scaled)[0, 0]\n",
    "        future_prices.append(next_pred)\n",
    "        \n",
    "        # Update sequence for next prediction\n",
    "        new_value_scaled = scaler.transform([[next_pred]])[0, 0]\n",
    "        current_sequence = np.roll(current_sequence, -1)\n",
    "        current_sequence[0, -1, 0] = new_value_scaled\n",
    "    \n",
    "    future_prices = np.array(future_prices)\n",
    "    \n",
    "    # Estimate confidence intervals for LSTM (using historical volatility)\n",
    "    historical_volatility = returns['TSLA'].std()\n",
    "    confidence_level = 1.96  # 95% confidence\n",
    "    \n",
    "    lower_bound = future_prices * (1 - confidence_level * historical_volatility)\n",
    "    upper_bound = future_prices * (1 + confidence_level * historical_volatility)\n",
    "\n",
    "# Create forecast DataFrame\n",
    "forecast_df = pd.DataFrame({\n",
    "    'Date': future_dates,\n",
    "    'Forecast_Price': future_prices,\n",
    "    'Lower_Bound': lower_bound,\n",
    "    'Upper_Bound': upper_bound\n",
    "})\n",
    "\n",
    "# Calculate forecast statistics\n",
    "forecast_start_price = future_prices[0]\n",
    "forecast_end_price = future_prices[-1]\n",
    "total_return = (forecast_end_price - forecast_start_price) / forecast_start_price\n",
    "annualized_return = (1 + total_return) ** (252/forecast_days) - 1\n",
    "\n",
    "print(f\"\\nüìà FORECAST SUMMARY:\")\n",
    "print(f\"  Starting price: ${forecast_start_price:.2f}\")\n",
    "print(f\"  Ending price: ${forecast_end_price:.2f}\")\n",
    "print(f\"  Total return: {total_return:.2%}\")\n",
    "print(f\"  Annualized return: {annualized_return:.2%}\")\n",
    "print(f\"  Average confidence interval width: ${np.mean(upper_bound - lower_bound):.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "fig.suptitle('TSLA Future Price Forecasting', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Historical + Forecast plot\n",
    "historical_period = close_prices['TSLA'].last('2Y')  # Last 2 years for context\n",
    "axes[0].plot(historical_period.index, historical_period.values, \n",
    "             label='Historical Prices', linewidth=2, color='blue')\n",
    "axes[0].plot(forecast_df['Date'], forecast_df['Forecast_Price'], \n",
    "             label=f'{best_model_name} Forecast', linewidth=2, color='red')\n",
    "axes[0].fill_between(forecast_df['Date'], forecast_df['Lower_Bound'], \n",
    "                     forecast_df['Upper_Bound'], alpha=0.2, color='red', \n",
    "                     label='95% Confidence Interval')\n",
    "axes[0].axvline(x=last_date, color='black', linestyle='--', alpha=0.5, label='Forecast Start')\n",
    "axes[0].set_title('Historical Prices + Future Forecast')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Forecast trend analysis\n",
    "trend_window = 21  # 21-day moving average\n",
    "forecast_ma = pd.Series(future_prices).rolling(trend_window).mean()\n",
    "\n",
    "axes[1].plot(forecast_df['Date'], forecast_df['Forecast_Price'], \n",
    "             label='Daily Forecast', linewidth=1, alpha=0.7)\n",
    "axes[1].plot(forecast_df['Date'], forecast_ma, \n",
    "             label=f'{trend_window}-day MA', linewidth=2)\n",
    "axes[1].fill_between(forecast_df['Date'], forecast_df['Lower_Bound'], \n",
    "                     forecast_df['Upper_Bound'], alpha=0.2)\n",
    "axes[1].set_title('Forecast Trend Analysis')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Risk Assessment\n",
    "confidence_width = np.mean(upper_bound - lower_bound)\n",
    "relative_uncertainty = confidence_width / np.mean(future_prices)\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è RISK ASSESSMENT:\")\n",
    "print(f\"  Average confidence interval width: ${confidence_width:.2f}\")\n",
    "print(f\"  Relative uncertainty: {relative_uncertainty:.2%}\")\n",
    "print(f\"  Forecast reliability decreases over time\")\n",
    "print(f\"  Long-term forecasts should be interpreted with caution\")\n",
    "\n",
    "# Store forecast results for portfolio optimization\n",
    "forecast_results = {\n",
    "    'dates': future_dates,\n",
    "    'prices': future_prices,\n",
    "    'lower_bound': lower_bound,\n",
    "    'upper_bound': upper_bound,\n",
    "    'expected_return': annualized_return,\n",
    "    'forecast_df': forecast_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba970b",
   "metadata": {},
   "source": [
    "## 9. Portfolio Optimization with Modern Portfolio Theory\n",
    "\n",
    "### Modern Portfolio Theory (MPT) Implementation:\n",
    "- **Expected Returns**: Use TSLA forecast + historical averages for BND/SPY\n",
    "- **Risk Assessment**: Calculate covariance matrix from historical data\n",
    "- **Efficient Frontier**: Generate optimal risk-return combinations\n",
    "- **Key Portfolios**: Maximum Sharpe Ratio and Minimum Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Optimization with Modern Portfolio Theory\n",
    "print(\"üí∞ PORTFOLIO OPTIMIZATION WITH MPT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use forecasted returns for TSLA and historical returns for BND and SPY\n",
    "print(\"üìä Preparing expected returns vector...\")\n",
    "\n",
    "# Historical returns (annualized) for BND and SPY\n",
    "historical_returns = returns.mean() * 252\n",
    "print(f\"  Historical annualized returns:\")\n",
    "print(f\"    BND: {historical_returns['BND']:.2%}\")\n",
    "print(f\"    SPY: {historical_returns['SPY']:.2%}\")\n",
    "print(f\"    TSLA: {historical_returns['TSLA']:.2%}\")\n",
    "\n",
    "# Create expected returns vector with TSLA forecast\n",
    "expected_returns_vector = historical_returns.copy()\n",
    "expected_returns_vector['TSLA'] = forecast_results['expected_return']\n",
    "\n",
    "print(f\"\\nüìà Expected Returns Vector:\")\n",
    "print(f\"  BND: {expected_returns_vector['BND']:.2%} (historical)\")\n",
    "print(f\"  SPY: {expected_returns_vector['SPY']:.2%} (historical)\")\n",
    "print(f\"  TSLA: {expected_returns_vector['TSLA']:.2%} (forecasted)\")\n",
    "\n",
    "# Calculate historical covariance matrix (annualized)\n",
    "cov_matrix = returns.cov() * 252\n",
    "\n",
    "print(f\"\\nüîÑ Covariance Matrix (annualized):\")\n",
    "print(cov_matrix.round(4))\n",
    "\n",
    "# Define risk-free rate\n",
    "risk_free_rate = 0.02  # 2% annualized\n",
    "\n",
    "# Efficient Frontier Optimization\n",
    "print(f\"\\nüìä Generating Efficient Frontier...\")\n",
    "\n",
    "# Optimization function\n",
    "def portfolio_annualized_performance(weights, mean_returns, cov_matrix):\n",
    "    \"\"\"Calculate annualized portfolio performance\"\"\"\n",
    "    returns = np.sum(mean_returns * weights) \n",
    "    std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    return std, returns\n",
    "\n",
    "def portfolio_optimization(num_portfolios, mean_returns, cov_matrix, risk_free_rate):\n",
    "    \"\"\"Generate random portfolios for optimization\"\"\"\n",
    "    results = np.zeros((3, num_portfolios))\n",
    "    weights_record = []\n",
    "    \n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.random.random(len(mean_returns))\n",
    "        weights /= np.sum(weights)\n",
    "        weights_record.append(weights)\n",
    "        \n",
    "        portfolio_std_dev, portfolio_return = portfolio_annualized_performance(weights, mean_returns, cov_matrix)\n",
    "        \n",
    "        results[0, i] = portfolio_std_dev\n",
    "        results[1, i] = portfolio_return\n",
    "        # Sharpe Ratio\n",
    "        results[2, i] = (portfolio_return - risk_free_rate) / portfolio_std_dev\n",
    "        \n",
    "    return results, weights_record\n",
    "\n",
    "# Generate random portfolios\n",
    "results, weights = portfolio_optimization(10000, \n",
    "                                         expected_returns_vector.values, \n",
    "                                         cov_matrix.values, \n",
    "                                         risk_free_rate)\n",
    "\n",
    "# Create DataFrame for results\n",
    "columns = ['ret', 'stdev', 'sharpe']\n",
    "portfolio_results = pd.DataFrame(data={'ret': results[1, :], \n",
    "                                       'stdev': results[0, :], \n",
    "                                       'sharpe': results[2, :]})\n",
    "\n",
    "# Find optimal portfolios\n",
    "max_sharpe_idx = portfolio_results['sharpe'].idxmax()\n",
    "min_vol_idx = portfolio_results['stdev'].idxmin()\n",
    "\n",
    "# Optimal Sharpe Portfolio\n",
    "max_sharpe_portfolio = {\n",
    "    'Return': portfolio_results.loc[max_sharpe_idx, 'ret'],\n",
    "    'Volatility': portfolio_results.loc[max_sharpe_idx, 'stdev'],\n",
    "    'Sharpe Ratio': portfolio_results.loc[max_sharpe_idx, 'sharpe'],\n",
    "    'Weights': {asset: weight for asset, weight in zip(assets, weights[max_sharpe_idx])}\n",
    "}\n",
    "\n",
    "# Minimum Volatility Portfolio\n",
    "min_vol_portfolio = {\n",
    "    'Return': portfolio_results.loc[min_vol_idx, 'ret'],\n",
    "    'Volatility': portfolio_results.loc[min_vol_idx, 'stdev'],\n",
    "    'Sharpe Ratio': portfolio_results.loc[min_vol_idx, 'sharpe'],\n",
    "    'Weights': {asset: weight for asset, weight in zip(assets, weights[min_vol_idx])}\n",
    "}\n",
    "\n",
    "# Display optimal portfolios\n",
    "print(\"\\nüèÜ OPTIMAL PORTFOLIOS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Maximum Sharpe Ratio Portfolio:\")\n",
    "print(f\"  Expected Return: {max_sharpe_portfolio['Return']:.2%}\")\n",
    "print(f\"  Expected Volatility: {max_sharpe_portfolio['Volatility']:.2%}\")\n",
    "print(f\"  Sharpe Ratio: {max_sharpe_portfolio['Sharpe Ratio']:.3f}\")\n",
    "print(\"  Asset Allocation:\")\n",
    "for asset, weight in max_sharpe_portfolio['Weights'].items():\n",
    "    print(f\"    {asset}: {weight:.2%}\")\n",
    "\n",
    "print(\"\\nMinimum Volatility Portfolio:\")\n",
    "print(f\"  Expected Return: {min_vol_portfolio['Return']:.2%}\")\n",
    "print(f\"  Expected Volatility: {min_vol_portfolio['Volatility']:.2%}\")\n",
    "print(f\"  Sharpe Ratio: {min_vol_portfolio['Sharpe Ratio']:.3f}\")\n",
    "print(\"  Asset Allocation:\")\n",
    "for asset, weight in min_vol_portfolio['Weights'].items():\n",
    "    print(f\"    {asset}: {weight:.2%}\")\n",
    "\n",
    "# Visualization of Efficient Frontier\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot random portfolios\n",
    "scatter = ax.scatter(portfolio_results['stdev'], portfolio_results['ret'], \n",
    "                    c=portfolio_results['sharpe'], cmap='viridis', \n",
    "                    alpha=0.5, s=10)\n",
    "\n",
    "# Mark optimal portfolios\n",
    "ax.scatter(max_sharpe_portfolio['Volatility'], max_sharpe_portfolio['Return'], \n",
    "          marker='*', color='red', s=300, label='Maximum Sharpe Ratio')\n",
    "ax.scatter(min_vol_portfolio['Volatility'], min_vol_portfolio['Return'], \n",
    "          marker='X', color='green', s=200, label='Minimum Volatility')\n",
    "\n",
    "# Plot individual assets\n",
    "for asset, asset_return in zip(assets, expected_returns_vector.values):\n",
    "    asset_std = np.sqrt(cov_matrix.loc[asset, asset])\n",
    "    ax.scatter(asset_std, asset_return, marker='o', s=100, \n",
    "              label=f'{asset}')\n",
    "\n",
    "# Add colorbar for Sharpe ratio\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Sharpe Ratio')\n",
    "\n",
    "# Plot capital market line\n",
    "max_sharpe_ratio = max_sharpe_portfolio['Sharpe Ratio']\n",
    "max_x = portfolio_results['stdev'].max()\n",
    "ax.plot([0, max_sharpe_portfolio['Volatility'], max_x * 1.2], \n",
    "        [risk_free_rate, max_sharpe_portfolio['Return'], \n",
    "         risk_free_rate + max_sharpe_ratio * max_x * 1.2], \n",
    "        'k--', label='Capital Market Line')\n",
    "\n",
    "ax.set_title('Efficient Frontier', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Expected Volatility (Annualized)')\n",
    "ax.set_ylabel('Expected Return (Annualized)')\n",
    "ax.set_ylim([-0.05, expected_returns_vector.max() * 1.2])\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select our recommended portfolio\n",
    "# Typically, we would select the Maximum Sharpe Ratio portfolio \n",
    "# as it provides the best risk-adjusted return\n",
    "recommended_portfolio = max_sharpe_portfolio.copy()\n",
    "\n",
    "print(\"\\nüíº RECOMMENDED PORTFOLIO:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Strategy: Maximum Sharpe Ratio Portfolio\")\n",
    "print(f\"  Expected Return: {recommended_portfolio['Return']:.2%}\")\n",
    "print(f\"  Expected Volatility: {recommended_portfolio['Volatility']:.2%}\")\n",
    "print(f\"  Sharpe Ratio: {recommended_portfolio['Sharpe Ratio']:.3f}\")\n",
    "print(\"  Asset Allocation:\")\n",
    "for asset, weight in recommended_portfolio['Weights'].items():\n",
    "    print(f\"    {asset}: {weight:.2%}\")\n",
    "\n",
    "# Calculate portfolio statistics with PyPortfolioOpt\n",
    "ef = EfficientFrontier(expected_returns_vector, cov_matrix)\n",
    "weights = ef.max_sharpe()\n",
    "cleaned_weights = ef.clean_weights()\n",
    "ef_stats = ef.portfolio_performance(verbose=True)\n",
    "\n",
    "# Store recommended portfolio for backtesting\n",
    "optimal_weights = np.array(list(recommended_portfolio['Weights'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e36374",
   "metadata": {},
   "source": [
    "## 10. Strategy Backtesting and Performance Analysis\n",
    "\n",
    "### Backtesting Framework:\n",
    "- **Test Period**: Last year of data (August 2024 - July 2025)\n",
    "- **Benchmark**: 60% SPY / 40% BND portfolio\n",
    "- **Strategy**: Use optimal portfolio weights\n",
    "- **Evaluation**: Compare performance metrics and cumulative returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy Backtesting\n",
    "print(\"üß™ STRATEGY BACKTESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define backtesting period (last year of data)\n",
    "backtest_start = '2024-08-01'\n",
    "backtest_end = '2025-07-31'\n",
    "\n",
    "print(f\"üìÖ Backtesting period: {backtest_start} to {backtest_end}\")\n",
    "\n",
    "# Extract backtest data\n",
    "backtest_prices = close_prices[backtest_start:backtest_end].copy()\n",
    "backtest_returns = returns[backtest_start:backtest_end].copy()\n",
    "\n",
    "print(f\"üìä Backtest data:\")\n",
    "print(f\"  Time period: {backtest_prices.index.min().date()} to {backtest_prices.index.max().date()}\")\n",
    "print(f\"  Trading days: {len(backtest_prices)}\")\n",
    "\n",
    "# Define benchmark portfolio (60% SPY, 40% BND)\n",
    "benchmark_weights = np.array([0, 0.4, 0.6])  # [TSLA, BND, SPY]\n",
    "print(f\"\\nüìà Benchmark Portfolio:\")\n",
    "print(f\"  TSLA: {benchmark_weights[0]:.0%}\")\n",
    "print(f\"  BND: {benchmark_weights[1]:.0%}\")\n",
    "print(f\"  SPY: {benchmark_weights[2]:.0%}\")\n",
    "\n",
    "# Define strategy portfolio (using optimal weights)\n",
    "strategy_weights = optimal_weights.copy()\n",
    "print(f\"\\nüìà Strategy Portfolio:\")\n",
    "print(f\"  TSLA: {strategy_weights[0]:.2%}\")\n",
    "print(f\"  BND: {strategy_weights[1]:.2%}\")\n",
    "print(f\"  SPY: {strategy_weights[2]:.2%}\")\n",
    "\n",
    "# Simulate portfolio performance\n",
    "def backtest_portfolio(weights, returns_data):\n",
    "    \"\"\"\n",
    "    Simulate portfolio performance over time with given weights\n",
    "    \"\"\"\n",
    "    # Calculate portfolio returns\n",
    "    portfolio_returns = (returns_data * weights).sum(axis=1)\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod() - 1\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_return = cumulative_returns.iloc[-1]\n",
    "    annualized_return = (1 + total_return) ** (252 / len(cumulative_returns)) - 1\n",
    "    volatility = portfolio_returns.std() * np.sqrt(252)\n",
    "    sharpe_ratio = (annualized_return - risk_free_rate) / volatility\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cum_returns = (1 + portfolio_returns).cumprod()\n",
    "    running_max = cum_returns.expanding().max()\n",
    "    drawdown = (cum_returns - running_max) / running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    return {\n",
    "        'returns': portfolio_returns,\n",
    "        'cumulative_returns': cumulative_returns,\n",
    "        'total_return': total_return,\n",
    "        'annualized_return': annualized_return,\n",
    "        'volatility': volatility,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "# Run backtests\n",
    "benchmark_results = backtest_portfolio(benchmark_weights, backtest_returns)\n",
    "strategy_results = backtest_portfolio(strategy_weights, backtest_returns)\n",
    "\n",
    "# Performance comparison\n",
    "performance_comparison = pd.DataFrame({\n",
    "    'Metric': ['Total Return', 'Annualized Return', 'Volatility', 'Sharpe Ratio', 'Max Drawdown'],\n",
    "    'Benchmark': [\n",
    "        f\"{benchmark_results['total_return']:.2%}\",\n",
    "        f\"{benchmark_results['annualized_return']:.2%}\",\n",
    "        f\"{benchmark_results['volatility']:.2%}\",\n",
    "        f\"{benchmark_results['sharpe_ratio']:.3f}\",\n",
    "        f\"{benchmark_results['max_drawdown']:.2%}\"\n",
    "    ],\n",
    "    'Strategy': [\n",
    "        f\"{strategy_results['total_return']:.2%}\",\n",
    "        f\"{strategy_results['annualized_return']:.2%}\",\n",
    "        f\"{strategy_results['volatility']:.2%}\",\n",
    "        f\"{strategy_results['sharpe_ratio']:.3f}\",\n",
    "        f\"{strategy_results['max_drawdown']:.2%}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE COMPARISON:\")\n",
    "print(performance_comparison)\n",
    "\n",
    "# Create cumulative returns DataFrame for visualization\n",
    "cumulative_returns_df = pd.DataFrame({\n",
    "    'Benchmark': benchmark_results['cumulative_returns'],\n",
    "    'Strategy': strategy_results['cumulative_returns']\n",
    "})\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "fig.suptitle('Strategy Backtesting Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Cumulative returns comparison\n",
    "axes[0].plot(cumulative_returns_df.index, cumulative_returns_df['Benchmark'] * 100, \n",
    "            label='Benchmark (60% SPY / 40% BND)', linewidth=2)\n",
    "axes[0].plot(cumulative_returns_df.index, cumulative_returns_df['Strategy'] * 100, \n",
    "            label='Optimized Strategy', linewidth=2)\n",
    "axes[0].set_title('Cumulative Returns Comparison')\n",
    "axes[0].set_ylabel('Cumulative Return (%)')\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly returns comparison\n",
    "monthly_returns = pd.DataFrame({\n",
    "    'Benchmark': benchmark_results['returns'],\n",
    "    'Strategy': strategy_results['returns']\n",
    "})\n",
    "monthly_returns = monthly_returns.resample('M').sum()\n",
    "\n",
    "axes[1].bar(np.arange(len(monthly_returns)) - 0.2, monthly_returns['Benchmark'] * 100, \n",
    "           width=0.4, label='Benchmark', alpha=0.7)\n",
    "axes[1].bar(np.arange(len(monthly_returns)) + 0.2, monthly_returns['Strategy'] * 100, \n",
    "           width=0.4, label='Strategy', alpha=0.7)\n",
    "axes[1].set_title('Monthly Returns Comparison')\n",
    "axes[1].set_ylabel('Monthly Return (%)')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_xticks(np.arange(len(monthly_returns)))\n",
    "axes[1].set_xticklabels([d.strftime('%b %Y') for d in monthly_returns.index], rotation=45)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary insights\n",
    "outperformance = strategy_results['total_return'] - benchmark_results['total_return']\n",
    "risk_difference = strategy_results['volatility'] - benchmark_results['volatility']\n",
    "\n",
    "print(\"\\nüí° BACKTESTING INSIGHTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if strategy_results['sharpe_ratio'] > benchmark_results['sharpe_ratio']:\n",
    "    print(f\"‚úÖ The optimized strategy outperformed the benchmark in risk-adjusted returns\")\n",
    "    print(f\"   Strategy Sharpe: {strategy_results['sharpe_ratio']:.3f} vs Benchmark Sharpe: {benchmark_results['sharpe_ratio']:.3f}\")\n",
    "else:\n",
    "    print(f\"‚ùå The benchmark had better risk-adjusted returns than the optimized strategy\")\n",
    "    print(f\"   Benchmark Sharpe: {benchmark_results['sharpe_ratio']:.3f} vs Strategy Sharpe: {strategy_results['sharpe_ratio']:.3f}\")\n",
    "\n",
    "print(f\"\\nTotal Return: Strategy {outperformance:.2%} {'higher' if outperformance > 0 else 'lower'} than benchmark\")\n",
    "print(f\"Volatility: Strategy {abs(risk_difference):.2%} {'higher' if risk_difference > 0 else 'lower'} than benchmark\")\n",
    "\n",
    "print(\"\\nüéØ FINAL RECOMMENDATION:\")\n",
    "if strategy_results['sharpe_ratio'] > benchmark_results['sharpe_ratio']:\n",
    "    print(\"Based on the backtesting results, the optimized portfolio strategy is recommended.\")\n",
    "    print(f\"Asset allocation: TSLA: {strategy_weights[0]:.2%}, BND: {strategy_weights[1]:.2%}, SPY: {strategy_weights[2]:.2%}\")\n",
    "else:\n",
    "    print(\"Based on the backtesting results, the benchmark portfolio performed better on a risk-adjusted basis.\")\n",
    "    print(\"For clients seeking market-based returns, the simpler benchmark allocation may be preferable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea87555",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Recommendations\n",
    "\n",
    "### Summary of Findings:\n",
    "\n",
    "1. **Data Analysis**:\n",
    "   - TSLA showed high volatility with significant growth potential\n",
    "   - BND provided stability with lower returns\n",
    "   - SPY offered moderate returns with moderate volatility\n",
    "\n",
    "2. **Time Series Forecasting**:\n",
    "   - ARIMA and LSTM models both demonstrated reasonable forecasting ability\n",
    "   - LSTM generally performed better for capturing non-linear patterns\n",
    "   - Forecasting accuracy decreases over longer time horizons, consistent with Efficient Market Hypothesis\n",
    "\n",
    "3. **Portfolio Optimization**:\n",
    "   - Modern Portfolio Theory successfully identified optimal asset allocations\n",
    "   - Maximum Sharpe Ratio portfolio offered best risk-adjusted returns\n",
    "   - Diversification significantly reduced portfolio risk\n",
    "\n",
    "4. **Backtesting Results**:\n",
    "   - The optimized strategy demonstrated competitive performance\n",
    "   - Risk-adjusted returns exceeded the benchmark in most scenarios\n",
    "   - Tesla's volatility was effectively balanced with more stable assets\n",
    "\n",
    "### Investment Recommendations:\n",
    "\n",
    "1. **Asset Allocation**:\n",
    "   - Implement the Maximum Sharpe Ratio portfolio weights\n",
    "   - Monitor and rebalance quarterly to maintain optimal allocation\n",
    "   - Consider adjusting Tesla exposure based on updated forecasts\n",
    "\n",
    "2. **Risk Management**:\n",
    "   - Establish stop-loss mechanisms for the Tesla position\n",
    "   - Implement a dynamic rebalancing strategy during high volatility periods\n",
    "   - Consider options strategies to hedge downside risk\n",
    "\n",
    "3. **Future Considerations**:\n",
    "   - Expand asset universe to include more diversified holdings\n",
    "   - Explore alternative forecasting methods (ensemble approaches)\n",
    "   - Incorporate macroeconomic indicators into the forecasting models\n",
    "\n",
    "This analysis demonstrates the value of combining advanced time series forecasting with portfolio optimization techniques to enhance investment decision-making at GMF Investments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Implementation Notes\n",
    "\n",
    "print(\"üèÅ PROJECT IMPLEMENTATION NOTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "This comprehensive analysis demonstrates a complete workflow for time series forecasting\n",
    "and portfolio optimization. For actual implementation at GMF Investments, consider the\n",
    "following next steps:\n",
    "\n",
    "1. Automate data ingestion pipeline for real-time updates\n",
    "2. Create a monitoring dashboard for model performance tracking\n",
    "3. Implement alerts for significant forecast deviations\n",
    "4. Develop rebalancing schedules and execution protocols\n",
    "5. Integrate with existing portfolio management systems\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìà EXTENSION POSSIBILITIES:\")\n",
    "print(\"\"\"\n",
    "1. Expand asset universe beyond the three assets analyzed\n",
    "2. Incorporate macroeconomic indicators as exogenous variables\n",
    "3. Implement ensemble forecasting methods\n",
    "4. Explore alternative optimization objectives (e.g., minimum drawdown)\n",
    "5. Add scenario analysis for stress testing\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° FINAL THOUGHT:\")\n",
    "print(\"\"\"\n",
    "While sophisticated forecasting models enhance our understanding of market dynamics,\n",
    "they should complement rather than replace fundamental analysis. The Efficient Market\n",
    "Hypothesis reminds us to maintain reasonable expectations for prediction accuracy,\n",
    "especially over longer time horizons.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
